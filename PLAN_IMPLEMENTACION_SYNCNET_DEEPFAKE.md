# Plan de Implementaci√≥n: Validador de Deepfake con SyncNet

**Fecha:** 2025-11-01
**Proyecto:** Dani Voice Assistant - AV-Sync Deepfake Detection
**Versi√≥n:** 1.0
**Estado:** Draft para Aprobaci√≥n

---

## üìã Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [An√°lisis del Estado Actual](#an√°lisis-del-estado-actual)
3. [Arquitectura Propuesta](#arquitectura-propuesta)
4. [Componentes a Desarrollar](#componentes-a-desarrollar)
5. [Setup de Infraestructura Local](#setup-de-infraestructura-local)
6. [Integraci√≥n con Flujo Actual](#integraci√≥n-con-flujo-actual)
7. [Fases de Implementaci√≥n](#fases-de-implementaci√≥n)
8. [Testing y Validaci√≥n](#testing-y-validaci√≥n)
9. [Consideraciones T√©cnicas](#consideraciones-t√©cnicas)
10. [Roadmap y Timeline](#roadmap-y-timeline)
11. [Riesgos y Mitigaciones](#riesgos-y-mitigaciones)

---

## 1. Resumen Ejecutivo

### Objetivo

Transformar el asistente de voz "Dani" en un **validador de deepfake en tiempo real** mediante la integraci√≥n de **SyncNet** para detectar sincron√≠a audio-visual (labios‚Üîvoz), ejecut√°ndose completamente en modo local sin infraestructura externa.

### Alcance MVP (Sprint 1-2)

- ‚úÖ Captura de video+audio durante interacci√≥n con el agente (3-4 segundos)
- ‚úÖ An√°lisis de sincron√≠a audio-labial con SyncNet (Python local)
- ‚úÖ Sistema de decisi√≥n con early-exit (ALLOW/NEXT/BLOCK)
- ‚úÖ UI/UX integrada con feedback visual del proceso
- ‚úÖ Ejecuci√≥n 100% local (sin dependencias cloud)

### M√©tricas de √âxito

| M√©trica | Objetivo |
|---------|----------|
| **T<sub>reto</sub>** | ‚â§ 15 segundos (instrucci√≥n + captura + an√°lisis) |
| **T<sub>E2E</sub>** | ‚â§ 25 segundos (incluyendo render de decisi√≥n) |
| **Early-exit rate** | ‚â• 60% en usuarios leg√≠timos |
| **False positive rate** | < 3-5% |
| **Latencia de an√°lisis** | < 8 segundos (p95) |

---

## 2. An√°lisis del Estado Actual

### 2.1 Stack Tecnol√≥gico Actual

**Frontend:**
```typescript
- React 18.3.1 + TypeScript 5.8.3
- Vite 5.4.19 (build tool)
- OpenAI Realtime API (WebRTC)
- Three.js 0.165.0 (visualizaci√≥n 3D)
- Shadcn/ui + Radix UI (componentes)
```

**Backend:**
```typescript
- Express 4.18.2 (servidor dev local)
- Vercel Serverless Functions (producci√≥n)
- Node.js API routes
```

**Arquitectura de Hooks Actual:**
```
useLirvana (orchestrator)
‚îú‚îÄ‚îÄ useWebRTC (gesti√≥n WebRTC + OpenAI)
‚îú‚îÄ‚îÄ useAudio (procesamiento audio navegador)
‚îî‚îÄ‚îÄ useSpeechRecognition (fallback Web Speech API)
```

### 2.2 Capacidades Existentes que Aprovechamos

‚úÖ **Ya implementado:**
- Captura de audio en tiempo real (WebRTC + MediaRecorder API)
- Gesti√≥n de permisos de micr√≥fono
- Sistema de herramientas (tools) extensible en `lirvanaTools.ts`
- Hook orchestrator (`useLirvana`) con arquitectura modular
- UI/UX con orbe 3D reactivo a audio
- Sistema de logging centralizado

‚ö†Ô∏è **Falta implementar:**
- Captura de video desde getUserMedia
- Servicio Python local para SyncNet
- API endpoint para an√°lisis de sincron√≠a
- Hook `useVideoCapture` para grabaci√≥n A/V
- Componente modal de captura con countdown
- L√≥gica de decisi√≥n con early-exit

---

## 3. Arquitectura Propuesta

### 3.1 Diagrama de Arquitectura (Local-First)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   BROWSER (React App)                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  UI Layer                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ AVSyncChallengeModal (nuevo)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ VideoPreview                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ Countdown (3-2-1)                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ ResultDisplay (‚úÖ‚ö†Ô∏è‚ùå)                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ ChatBox (existente - agregar trigger)        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                     ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Hook Layer                                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ useLirvana (orchestrator) ‚Üê EXTENDER         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ useWebRTC (existente)                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ useAudio (existente)                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ useVideoCapture (NUEVO)                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ      ‚îú‚îÄ‚îÄ getUserMedia(audio+video)                ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ      ‚îú‚îÄ‚îÄ MediaRecorder ‚Üí Blob                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ      ‚îî‚îÄ‚îÄ uploadToLocal()                          ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                     ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Service Layer                                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ lirvanaTools.ts ‚Üê AGREGAR av_sync_tool       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ avSyncService.ts (NUEVO)                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ      ‚îî‚îÄ‚îÄ POST /api/avsync/score                   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTP POST (FormData)
                     ‚îÇ video: Blob (WebM/MP4)
                     ‚îÇ sessionId: string
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LOCALHOST:3001 (Express Server)                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  API Route: /api/avsync/score                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Validar request (MIME, duraci√≥n)         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Guardar clip ‚Üí /tmp/uploads/             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ Forward ‚Üí Python Service                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTP POST
                     ‚îÇ {videoPath: string}
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     LOCALHOST:5000 (Python Flask/FastAPI)                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Endpoint: POST /score                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Extraer frames/audio con ffmpeg          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Detectar cara (S3FD)                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ SyncNet forward pass                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ Calcular m√©tricas                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ      {offset, confidence, min_dist, score}    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  SyncNet Pipeline (joonson/syncnet_python)    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Models: syncnet_v2.model, sfd_face.pth   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ Scripts: run_syncnet.py wrapper          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ Dependencies: PyTorch, ffmpeg, opencv    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Flujo de Datos

```mermaid
sequenceDiagram
    participant U as Usuario
    participant A as Agente Realtime
    participant C as AVSyncChallengeModal
    participant H as useVideoCapture
    participant API as Express API
    participant P as Python Service
    participant S as SyncNet

    A->>U: "Verifica tu identidad. Di: 'para Paco pinta picos'"
    U->>C: Acepta reto
    C->>H: startCapture()
    H->>U: getUserMedia(audio+video)
    U-->>H: MediaStream
    H->>C: Countdown 3-2-1 (UI)
    U->>U: Habla frase (3-4s)
    H->>H: MediaRecorder ‚Üí Blob (WebM/MP4)
    H->>API: POST /api/avsync/score (FormData)
    API->>API: Validar y guardar en /tmp
    API->>P: POST /score {videoPath}
    P->>S: run_syncnet.py
    S->>P: {offset, confidence, min_dist}
    P->>API: {offset, confidence, min_dist, score: 0.91}
    API->>H: {score: 0.91, decision: "ALLOW"}
    H->>C: Mostrar resultado ‚úÖ
    C->>A: Continuar conversaci√≥n (early-exit)
```

### 3.3 Componentes del Sistema

| Layer | Componente | Tipo | Estado |
|-------|-----------|------|--------|
| **UI** | `AVSyncChallengeModal.tsx` | Component | üÜï Nuevo |
| **UI** | `VideoPreview.tsx` | Component | üÜï Nuevo |
| **UI** | `ResultFeedback.tsx` | Component | üÜï Nuevo |
| **Hook** | `useVideoCapture.ts` | Custom Hook | üÜï Nuevo |
| **Hook** | `useLirvana.ts` | Orchestrator | üîß Extender |
| **Service** | `avSyncService.ts` | TS Service | üÜï Nuevo |
| **Service** | `lirvanaTools.ts` | TS Service | üîß Extender |
| **API** | `/api/avsync/score` | Express Route | üÜï Nuevo |
| **Backend** | `syncnet-service` | Python Flask | üÜï Nuevo |
| **Backend** | `syncnet_wrapper.py` | Python Script | üÜï Nuevo |

---

## 4. Componentes a Desarrollar

### 4.1 Frontend - Nuevos Componentes

#### **AVSyncChallengeModal.tsx**

Modal principal del reto de sincron√≠a.

**Props:**
```typescript
interface AVSyncChallengeModalProps {
  isOpen: boolean;
  onClose: () => void;
  onComplete: (result: AVSyncResult) => void;
  challengePhrase?: string;
  agentVoice: (text: string) => void;
}

interface AVSyncResult {
  score: number;           // 0-1
  offset_frames: number;
  confidence: number;
  min_dist: number;
  lag_ms: number;
  decision: 'ALLOW' | 'NEXT' | 'BLOCK';
  timestamp: string;
}
```

**Estados:**
```typescript
type ChallengeState =
  | 'instructions'    // Mostrar instrucciones
  | 'countdown'       // 3-2-1
  | 'recording'       // Grabando (3-4s)
  | 'processing'      // Spinner + "Analizando..."
  | 'result';         // Mostrar resultado
```

**Funcionalidades:**
- ‚úÖ Mostrar frase a repetir
- ‚úÖ Countdown visual animado (3-2-1)
- ‚úÖ Preview de c√°mara en tiempo real
- ‚úÖ Indicador de grabaci√≥n activa
- ‚úÖ Spinner durante an√°lisis
- ‚úÖ Resultado con iconos (‚úÖ‚ö†Ô∏è‚ùå) + m√©tricas
- ‚úÖ Botones: Reintentar / Continuar

#### **VideoPreview.tsx**

Componente de preview de c√°mara.

```typescript
interface VideoPreviewProps {
  stream: MediaStream | null;
  isRecording: boolean;
  countdown: number | null;
  onPermissionDenied: () => void;
}
```

**Caracter√≠sticas:**
- Video element con `srcObject = stream`
- Overlay circular para detectar rostro
- Badge de estado (üî¥ GRABANDO / ‚è∏Ô∏è PAUSA)
- Manejo de errores de permisos

#### **ResultFeedback.tsx**

Display de resultados del an√°lisis.

```typescript
interface ResultFeedbackProps {
  result: AVSyncResult;
  onRetry: () => void;
  onContinue: () => void;
}
```

**UI por decisi√≥n:**

| Decision | Icon | Color | Message | Actions |
|----------|------|-------|---------|---------|
| `ALLOW` | ‚úÖ | Verde | "Verificaci√≥n exitosa" | [Continuar] |
| `NEXT` | ‚ö†Ô∏è | √Åmbar | "Verificaci√≥n inconclusa. Reto adicional" | [Siguiente Reto] |
| `BLOCK` | ‚ùå | Rojo | "Alto riesgo detectado" | [Reintentar] [WebAuthn] |

**M√©tricas mostradas:**
```
Score: 91/100 ‚úÖ
Sincron√≠a: +3 frames (100ms)
Confianza: 10.02
```

### 4.2 Frontend - Nuevo Hook

#### **useVideoCapture.ts**

Hook para captura de video+audio.

```typescript
interface UseVideoCaptureConfig {
  duration?: number;          // Default: 4000ms
  mimeType?: string;          // Default: 'video/webm;codecs=vp9,opus'
  videoBitsPerSecond?: number; // Default: 2500000
  constraints?: MediaStreamConstraints;
}

interface UseVideoCaptureReturn {
  // State
  stream: MediaStream | null;
  isCapturing: boolean;
  isPaused: boolean;
  countdown: number | null;
  recordedBlob: Blob | null;
  error: string | null;

  // Methods
  requestPermissions: () => Promise<void>;
  startCapture: () => Promise<void>;
  stopCapture: () => void;
  pauseCapture: () => void;
  resumeCapture: () => void;
  resetCapture: () => void;

  // Utils
  downloadBlob: () => void;
  getVideoUrl: () => string | null;
}
```

**Implementaci√≥n clave:**

```typescript
const startCapture = async () => {
  try {
    // 1. getUserMedia
    const mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        sampleRate: 24000,
      },
      video: {
        width: { ideal: 640 },
        height: { ideal: 480 },
        frameRate: { ideal: 30 },
        facingMode: 'user',
      },
    });

    setStream(mediaStream);

    // 2. Countdown
    for (let i = 3; i > 0; i--) {
      setCountdown(i);
      await sleep(1000);
    }
    setCountdown(null);

    // 3. MediaRecorder
    const recorder = new MediaRecorder(mediaStream, {
      mimeType: config.mimeType,
      videoBitsPerSecond: config.videoBitsPerSecond,
    });

    const chunks: Blob[] = [];
    recorder.ondataavailable = (e) => chunks.push(e.data);
    recorder.onstop = () => {
      const blob = new Blob(chunks, { type: config.mimeType });
      setRecordedBlob(blob);
      onComplete?.(blob);
    };

    recorder.start();
    setIsCapturing(true);

    // 4. Auto-stop despu√©s de `duration`
    setTimeout(() => {
      recorder.stop();
      mediaStream.getTracks().forEach(track => track.stop());
      setIsCapturing(false);
    }, config.duration);

  } catch (err) {
    setError(err.message);
    throw err;
  }
};
```

### 4.3 Frontend - Servicio

#### **avSyncService.ts**

Servicio para comunicaci√≥n con API de an√°lisis.

```typescript
export interface AVSyncScoreRequest {
  videoBlob: Blob;
  sessionId: string;
  userConsent: boolean;
  metadata?: {
    challengePhrase?: string;
    timestamp: string;
    userAgent: string;
  };
}

export interface AVSyncScoreResponse {
  offset_frames: number;
  confidence: number;
  min_dist: number;
  score: number;           // 0-1 normalizado
  lag_ms: number;
  decision: 'ALLOW' | 'NEXT' | 'BLOCK';
  ttl_ms: number;
  reason_codes?: string[];
  debug?: {
    pywork_path: string;
    pycrop_path: string;
  };
}

export class AVSyncService {
  private baseURL: string;

  constructor() {
    this.baseURL = import.meta.env.MODE === 'production'
      ? window.location.origin
      : import.meta.env.VITE_API_BASE_URL || 'http://localhost:3001';
  }

  async scoreVideo(request: AVSyncScoreRequest): Promise<AVSyncScoreResponse> {
    const formData = new FormData();
    formData.append('video', request.videoBlob, 'capture.webm');
    formData.append('session_id', request.sessionId);
    formData.append('user_consent', request.userConsent.toString());
    if (request.metadata) {
      formData.append('metadata', JSON.stringify(request.metadata));
    }

    try {
      const response = await fetch(`${this.baseURL}/api/avsync/score`, {
        method: 'POST',
        body: formData,
        // No headers - browser sets Content-Type con boundary
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error || `HTTP ${response.status}`);
      }

      return await response.json();
    } catch (error) {
      console.error('[AVSyncService] scoreVideo error:', error);
      throw error;
    }
  }

  /**
   * Normalizar m√©tricas de SyncNet a score 0-1
   * Formula: sigmoid(a * confidence - b * min_dist - c * |offset|)
   */
  private normalizeScore(
    confidence: number,
    min_dist: number,
    offset_frames: number
  ): number {
    // Pesos calibrados (ajustar con dataset)
    const a = 0.15;
    const b = 0.25;
    const c = 0.1;

    const z = a * confidence - b * min_dist - c * Math.abs(offset_frames);
    return 1 / (1 + Math.exp(-z)); // Sigmoid
  }

  /**
   * Aplicar l√≥gica de decisi√≥n
   */
  makeDecision(score: number, offset_frames: number): 'ALLOW' | 'NEXT' | 'BLOCK' {
    if (score >= 0.90 && Math.abs(offset_frames) <= 2) {
      return 'ALLOW';  // Early-exit
    } else if (score >= 0.75) {
      return 'NEXT';   // Siguiente reto
    } else {
      return 'BLOCK';  // Alto riesgo
    }
  }
}
```

### 4.4 Backend - Express API

#### **/server/api/avsync.js** (nuevo)

```javascript
const express = require('express');
const multer = require('multer');
const path = require('path');
const fs = require('fs').promises;
const fetch = require('node-fetch');

const router = express.Router();

// Configurar multer para uploads temporales
const upload = multer({
  dest: path.join(__dirname, '../../tmp/uploads/'),
  limits: {
    fileSize: 10 * 1024 * 1024, // 10 MB
  },
  fileFilter: (req, file, cb) => {
    // Solo video
    if (file.mimetype.startsWith('video/')) {
      cb(null, true);
    } else {
      cb(new Error('Solo archivos de video permitidos'), false);
    }
  },
});

/**
 * POST /api/avsync/score
 * Analizar sincron√≠a audio-visual de un clip
 */
router.post('/score', upload.single('video'), async (req, res) => {
  const { file, body } = req;

  try {
    // Validar request
    if (!file) {
      return res.status(400).json({ error: 'No se proporcion√≥ video' });
    }

    const { session_id, user_consent } = body;

    if (!session_id) {
      return res.status(400).json({ error: 'session_id requerido' });
    }

    if (user_consent !== 'true') {
      return res.status(403).json({ error: 'Consentimiento requerido' });
    }

    // Validar duraci√≥n (debe ser 2-6 segundos)
    // TODO: usar ffprobe para verificar duraci√≥n

    console.log('[AVSync] Processing video:', {
      originalName: file.originalname,
      size: file.size,
      path: file.path,
      sessionId: session_id,
    });

    // Forward a Python service
    const pythonServiceURL = process.env.PYTHON_SERVICE_URL || 'http://localhost:5000';

    const pythonResponse = await fetch(`${pythonServiceURL}/score`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        video_path: file.path,
        session_id: session_id,
      }),
    });

    if (!pythonResponse.ok) {
      const errorData = await pythonResponse.json().catch(() => ({}));
      throw new Error(errorData.error || `Python service error: ${pythonResponse.status}`);
    }

    const result = await pythonResponse.json();

    // Limpiar archivo temporal (opcional: mantener 24h para debugging)
    if (process.env.CLEANUP_UPLOADS === 'true') {
      await fs.unlink(file.path).catch(console.error);
    }

    // Agregar decisi√≥n
    const decision = makeDecision(result.score, result.offset_frames);

    res.json({
      ...result,
      decision,
      ttl_ms: Date.now() + 300000, // 5 min TTL
      session_id,
    });

  } catch (error) {
    console.error('[AVSync] Error:', error);
    res.status(500).json({
      error: 'Error procesando video',
      message: error.message,
    });
  }
});

/**
 * L√≥gica de decisi√≥n (early-exit)
 */
function makeDecision(score, offset_frames) {
  if (score >= 0.90 && Math.abs(offset_frames) <= 2) {
    return 'ALLOW';
  } else if (score >= 0.75) {
    return 'NEXT';
  } else {
    return 'BLOCK';
  }
}

module.exports = router;
```

**Registrar en server/index.js:**

```javascript
const avSyncRouter = require('./api/avsync');
app.use('/api/avsync', avSyncRouter);
```

### 4.5 Backend - Python Service

#### **Estructura del servicio Python**

```
syncnet-service/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ app.py                  # Flask/FastAPI server
‚îú‚îÄ‚îÄ syncnet_wrapper.py      # Wrapper de SyncNet
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ syncnet_v2.model
‚îÇ   ‚îî‚îÄ‚îÄ sfd_face.pth
‚îú‚îÄ‚îÄ syncnet_python/         # Submodule de joonson/syncnet_python
‚îÇ   ‚îú‚îÄ‚îÄ SyncNetModel.py
‚îÇ   ‚îú‚îÄ‚îÄ SyncNetInstance.py
‚îÇ   ‚îú‚îÄ‚îÄ run_syncnet.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ tmp/                    # Outputs temporales
    ‚îú‚îÄ‚îÄ pywork/
    ‚îú‚îÄ‚îÄ pycrop/
    ‚îî‚îÄ‚îÄ pyavi/
```

#### **requirements.txt**

```txt
# Core
flask==3.0.0
flask-cors==4.0.0
python-dotenv==1.0.0

# SyncNet dependencies
torch>=2.0.0
torchvision>=0.15.0
opencv-python==4.8.1.78
numpy>=1.24.0
scipy>=1.11.0
scikit-image>=0.21.0
tqdm>=4.66.0

# Face detection
facenet-pytorch>=2.5.3

# Utils
python-multipart>=0.0.6
```

#### **app.py** (Flask server)

```python
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import logging
from syncnet_wrapper import SyncNetWrapper

app = Flask(__name__)
CORS(app)

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Inicializar SyncNet
syncnet = SyncNetWrapper(
    model_path='./models/syncnet_v2.model',
    detector_path='./models/sfd_face.pth',
    tmp_dir='./tmp'
)

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'syncnet-avsync',
        'version': '1.0.0'
    })

@app.route('/score', methods=['POST'])
def score_video():
    """
    Analizar sincron√≠a audio-visual de un video

    Request:
    {
        "video_path": "/tmp/uploads/abc123.webm",
        "session_id": "sess_xyz"
    }

    Response:
    {
        "offset_frames": 3,
        "confidence": 10.02,
        "min_dist": 5.35,
        "score": 0.91,
        "lag_ms": 100,
        "processing_time_ms": 7234
    }
    """
    try:
        data = request.get_json()

        video_path = data.get('video_path')
        session_id = data.get('session_id')

        if not video_path or not os.path.exists(video_path):
            return jsonify({'error': 'video_path inv√°lido o no existe'}), 400

        logger.info(f'[{session_id}] Processing video: {video_path}')

        # Ejecutar SyncNet
        result = syncnet.process_video(video_path, session_id)

        logger.info(f'[{session_id}] Result: score={result["score"]:.3f}, offset={result["offset_frames"]}')

        return jsonify(result)

    except Exception as e:
        logger.error(f'Error processing video: {str(e)}', exc_info=True)
        return jsonify({
            'error': 'Error procesando video',
            'message': str(e)
        }), 500

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
```

#### **syncnet_wrapper.py**

```python
import os
import sys
import time
import subprocess
import numpy as np
from pathlib import Path

# Agregar syncnet_python al path
sys.path.append('./syncnet_python')

from SyncNetInstance import SyncNetInstance

class SyncNetWrapper:
    """Wrapper para SyncNet con API simplificada"""

    def __init__(self, model_path: str, detector_path: str, tmp_dir: str = './tmp'):
        self.model_path = model_path
        self.detector_path = detector_path
        self.tmp_dir = Path(tmp_dir)

        # Crear directorios
        (self.tmp_dir / 'pywork').mkdir(parents=True, exist_ok=True)
        (self.tmp_dir / 'pycrop').mkdir(parents=True, exist_ok=True)
        (self.tmp_dir / 'pyavi').mkdir(parents=True, exist_ok=True)

        # Inicializar modelo
        self.syncnet = SyncNetInstance()
        self.syncnet.loadParameters(model_path)

        print(f'[SyncNet] Modelo cargado: {model_path}')

    def process_video(self, video_path: str, reference: str) -> dict:
        """
        Procesar video y retornar m√©tricas de sincron√≠a

        Args:
            video_path: Ruta al video a analizar
            reference: ID de referencia (ej. session_id)

        Returns:
            {
                'offset_frames': int,
                'confidence': float,
                'min_dist': float,
                'score': float,        # 0-1 normalizado
                'lag_ms': float,
                'processing_time_ms': int,
                'debug': {...}
            }
        """
        start_time = time.time()

        try:
            # 1. Ejecutar pipeline de SyncNet
            # Alternativa: usar run_pipeline.py + run_syncnet.py como scripts
            # Por simplicidad, usamos comandos directos

            data_dir = str(self.tmp_dir)

            # Pipeline completo (extraer frames, detectar cara, calcular offset)
            cmd = [
                'python', './syncnet_python/run_pipeline.py',
                '--videofile', video_path,
                '--reference', reference,
                '--data_dir', data_dir
            ]

            subprocess.run(cmd, check=True, capture_output=True)

            # Leer offsets.txt
            offsets_path = self.tmp_dir / 'pywork' / reference / 'offsets.txt'

            if not offsets_path.exists():
                raise FileNotFoundError(f'No se gener√≥ offsets.txt: {offsets_path}')

            # Parsear offsets.txt
            # Formato: cada l√≠nea es "offset conf min_dist"
            offsets_data = []
            with open(offsets_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 3:
                        offset, conf, min_dist = map(float, parts)
                        offsets_data.append((offset, conf, min_dist))

            if not offsets_data:
                raise ValueError('offsets.txt est√° vac√≠o')

            # Tomar mejor resultado (mayor confianza)
            best = max(offsets_data, key=lambda x: x[1])
            offset_frames, confidence, min_dist = best

            # Calcular lag en ms (asumiendo 25 fps)
            fps = 25
            lag_ms = (offset_frames / fps) * 1000

            # Normalizar a score 0-1
            score = self._normalize_score(confidence, min_dist, offset_frames)

            processing_time = int((time.time() - start_time) * 1000)

            return {
                'offset_frames': int(offset_frames),
                'confidence': round(confidence, 3),
                'min_dist': round(min_dist, 3),
                'score': round(score, 4),
                'lag_ms': round(lag_ms, 1),
                'processing_time_ms': processing_time,
                'debug': {
                    'pywork_path': str(self.tmp_dir / 'pywork' / reference),
                    'pycrop_path': str(self.tmp_dir / 'pycrop' / reference),
                    'num_results': len(offsets_data)
                }
            }

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f'Error ejecutando SyncNet: {e.stderr.decode()}')
        except Exception as e:
            raise RuntimeError(f'Error en pipeline: {str(e)}')

    def _normalize_score(self, confidence: float, min_dist: float, offset_frames: float) -> float:
        """
        Normalizar m√©tricas a score 0-1
        Formula: sigmoid(a * confidence - b * min_dist - c * |offset|)

        Pesos iniciales (calibrar con dataset):
        - a = 0.15 (peso confidence)
        - b = 0.25 (peso min_dist)
        - c = 0.10 (peso offset)
        """
        a, b, c = 0.15, 0.25, 0.10
        z = a * confidence - b * min_dist - c * abs(offset_frames)

        # Sigmoid
        score = 1 / (1 + np.exp(-z))
        return float(np.clip(score, 0.0, 1.0))
```

#### **Instalaci√≥n y Setup**

```bash
# 1. Clonar repo SyncNet
cd syncnet-service
git clone https://github.com/joonson/syncnet_python.git

# 2. Descargar modelos
cd syncnet_python
sh download_model.sh
# Esto descarga syncnet_v2.model y sfd_face.pth

# Mover modelos a carpeta models/
mv data/syncnet_v2.model ../models/
mv data/sfd_face.pth ../models/

# 3. Instalar ffmpeg (requisito cr√≠tico)
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y ffmpeg

# macOS:
brew install ffmpeg

# 4. Crear entorno virtual Python
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 5. Instalar dependencias
pip install -r requirements.txt

# 6. Verificar instalaci√≥n
python -c "import torch; print(torch.__version__)"
ffmpeg -version

# 7. Ejecutar servidor
python app.py
# Server running on http://localhost:5000
```

---

## 5. Setup de Infraestructura Local

### 5.1 Configuraci√≥n de Desarrollo

**Variables de entorno (.env):**

```bash
# OpenAI (existente)
VITE_OPENAI_API_KEY=sk-proj-...
VITE_REALTIME_MODEL=gpt-4o-realtime-preview

# AVSync Service
PYTHON_SERVICE_URL=http://localhost:5000
CLEANUP_UPLOADS=false  # true en producci√≥n

# Flask (syncnet-service/.env)
PORT=5000
FLASK_ENV=development
MODEL_PATH=./models/syncnet_v2.model
DETECTOR_PATH=./models/sfd_face.pth
TMP_DIR=./tmp
```

**Scripts en package.json:**

```json
{
  "scripts": {
    "dev": "vite",
    "dev:server": "cd server && npm run dev",
    "dev:python": "cd syncnet-service && source venv/bin/activate && python app.py",
    "dev:full": "concurrently \"npm run dev:server\" \"npm run dev:python\" \"npm run dev\"",
    "build": "vite build",
    "preview": "vite preview"
  }
}
```

**Iniciar stack completo:**

```bash
# Terminal 1: Frontend (Vite)
npm run dev

# Terminal 2: Backend Express
npm run dev:server

# Terminal 3: Python Service
cd syncnet-service
source venv/bin/activate
python app.py

# O todo junto:
npm run dev:full
```

### 5.2 Requisitos del Sistema

| Componente | Requisito M√≠nimo | Recomendado |
|------------|------------------|-------------|
| **OS** | Ubuntu 20.04+ / macOS 11+ / Windows 10+ | Ubuntu 22.04 LTS |
| **Node.js** | 18.x | 20.x LTS |
| **Python** | 3.8+ | 3.10+ |
| **RAM** | 4 GB | 8 GB |
| **Disk** | 2 GB (modelos) | 5 GB |
| **ffmpeg** | 4.x | 5.x |
| **GPU** | Opcional (CPU basta) | CUDA 11.8+ para acelerar |

### 5.3 Instalaci√≥n de Dependencias

**ffmpeg (CR√çTICO):**

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y ffmpeg

# macOS
brew install ffmpeg

# Windows (con Chocolatey)
choco install ffmpeg

# Verificar
ffmpeg -version
```

**Python Dependencies:**

```bash
cd syncnet-service
python3 -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install -r requirements.txt

# Verificar PyTorch
python -c "import torch; print(f'PyTorch {torch.__version__} (CUDA: {torch.cuda.is_available()})')"
```

**Node Dependencies:**

```bash
# Frontend
npm install

# Backend
cd server && npm install

# Instalar multer para uploads
cd server && npm install multer node-fetch
```

---

## 6. Integraci√≥n con Flujo Actual

### 6.1 Modificaci√≥n de useLirvana

Agregar nuevo tool `av_sync_challenge`:

```typescript
// src/hooks/useLirvana.ts

import { AVSyncService } from '@/services/avSyncService';

export function useLirvana(config: UseLirvanaConfig = {}): UseLirvanaReturn {
  // ... c√≥digo existente ...

  const [isChallengeActive, setIsChallengeActive] = useState(false);
  const avSyncServiceRef = useRef(new AVSyncService());

  // Registrar tool av_sync_challenge
  useEffect(() => {
    if (toolsRef.current && webrtc.isSessionActive) {
      // ... tools existentes ...

      // Nuevo tool
      webrtc.registerFunction('av_sync_challenge', async (params: any) => {
        try {
          log('info', 'Iniciando reto AV-Sync', params);

          // Activar modal de captura
          setIsChallengeActive(true);

          // Esperar resultado (manejado por callback en modal)
          return {
            success: true,
            message: 'Reto iniciado. Por favor, sigue las instrucciones en pantalla.',
            challenge_id: uuidv4(),
          };
        } catch (error: any) {
          log('error', 'Error en reto AV-Sync', error);
          return {
            success: false,
            error: error.message,
          };
        }
      });
    }
  }, [webrtc.isSessionActive]);

  // Handler para resultado del modal
  const handleChallengeComplete = useCallback(async (result: AVSyncResult) => {
    setIsChallengeActive(false);

    if (result.decision === 'ALLOW') {
      // Early-exit: continuar conversaci√≥n
      webrtc.sendTextMessage('Verificaci√≥n exitosa, continuemos.');
    } else if (result.decision === 'NEXT') {
      // Siguiente reto (placeholder)
      webrtc.sendTextMessage('Necesito verificar con un reto adicional.');
    } else {
      // BLOCK
      webrtc.sendTextMessage('No pude verificar tu identidad. Intenta de nuevo o usa otro m√©todo.');
    }
  }, [webrtc.sendTextMessage]);

  return {
    // ... retorno existente ...
    isChallengeActive,
    handleChallengeComplete,
  };
}
```

### 6.2 Modificaci√≥n de lirvanaTools.ts

Agregar definici√≥n del tool:

```typescript
// src/services/lirvanaTools.ts

getTools(): RealtimeTool[] {
  return [
    // ... tools existentes ...

    {
      name: 'av_sync_challenge',
      description: 'Iniciar reto de verificaci√≥n de sincron√≠a audio-visual para detectar deepfakes. Usar cuando se requiera validar la identidad del usuario o detectar posible suplantaci√≥n.',
      parameters: {
        type: 'object',
        properties: {
          challenge_phrase: {
            type: 'string',
            description: 'Frase que el usuario debe repetir (opcional, por defecto se usa una frase aleatoria)',
          },
          difficulty: {
            type: 'string',
            enum: ['easy', 'medium', 'hard'],
            description: 'Dificultad del reto (easy: frase corta, hard: frase larga con trabalenguas)',
          },
        },
        required: [],
      },
    },
  ];
}
```

### 6.3 Integraci√≥n en ChatBox

Agregar modal en `ChatBox.tsx`:

```typescript
// src/components/ChatBox.tsx

import AVSyncChallengeModal from './AVSyncChallengeModal';

export function ChatBox() {
  const {
    // ... props existentes ...
    isChallengeActive,
    handleChallengeComplete,
  } = useLirvana();

  return (
    <div className="chat-box">
      {/* ... UI existente ... */}

      {/* Modal de reto AV-Sync */}
      <AVSyncChallengeModal
        isOpen={isChallengeActive}
        onClose={() => {/* handle cancel */}}
        onComplete={handleChallengeComplete}
        challengePhrase="Para Paco pinta picos"
        agentVoice={(text) => {/* TTS con Web Speech API */}}
      />
    </div>
  );
}
```

### 6.4 Flujo de Usuario (Happy Path)

```
1. Usuario: "Hola, quiero comprar un panel solar"
   ‚Üì
2. Agente: "Claro, con gusto. Antes, necesito verificar tu identidad."
   ‚Üì
3. Agente ejecuta tool: av_sync_challenge()
   ‚Üì
4. Modal se abre ‚Üí Instrucciones: "Di en voz alta: 'para Paco pinta picos'"
   ‚Üì
5. Countdown: 3... 2... 1... üî¥
   ‚Üì
6. Usuario habla durante 4 segundos
   ‚Üì
7. Captura ‚Üí Blob (WebM) ‚Üí POST /api/avsync/score
   ‚Üì
8. Express ‚Üí Python ‚Üí SyncNet ‚Üí {score: 0.92, offset: 1, decision: "ALLOW"}
   ‚Üì
9. Modal muestra: ‚úÖ "Verificaci√≥n exitosa" (score: 92/100)
   ‚Üì
10. Modal se cierra (early-exit)
    ‚Üì
11. Agente: "Perfecto! Ahora cu√©ntame, ¬øqu√© panel te interesa?"
    ‚Üì
12. Conversaci√≥n contin√∫a normalmente...
```

---

## 7. Fases de Implementaci√≥n

### **Fase 1: Setup de Infraestructura (D√≠as 1-2)**

**Objetivos:**
- ‚úÖ Instalar SyncNet localmente
- ‚úÖ Configurar servidor Python (Flask)
- ‚úÖ Verificar pipeline b√°sico

**Tareas:**

```bash
[P1.1] Crear carpeta syncnet-service/
[P1.2] Clonar repo joonson/syncnet_python
[P1.3] Descargar modelos (download_model.sh)
[P1.4] Instalar dependencias (requirements.txt)
[P1.5] Crear app.py y syncnet_wrapper.py
[P1.6] Test manual: curl POST /score con video de prueba
[P1.7] Documentar setup en README_SYNCNET.md
```

**Criterios de Aceptaci√≥n:**
- `python app.py` inicia sin errores
- `/health` endpoint responde 200
- `/score` endpoint procesa un video de prueba y retorna `{offset, confidence, score}`

---

### **Fase 2: Backend API (D√≠as 3-4)**

**Objetivos:**
- ‚úÖ Crear endpoint Express `/api/avsync/score`
- ‚úÖ Integrar con Python service
- ‚úÖ Validaci√≥n de inputs

**Tareas:**

```bash
[P2.1] Instalar multer en server/
[P2.2] Crear /server/api/avsync.js
[P2.3] Implementar upload de video + validaci√≥n
[P2.4] Forward a Python service (fetch POST /score)
[P2.5] Agregar l√≥gica de decisi√≥n (ALLOW/NEXT/BLOCK)
[P2.6] Manejo de errores + logging
[P2.7] Test con Postman/curl
```

**Criterios de Aceptaci√≥n:**
- `POST /api/avsync/score` acepta FormData con video
- Responde con `{score, decision, ttl_ms}` en < 10s (p95)
- Logs detallados en consola

---

### **Fase 3: Frontend - Hook y Servicio (D√≠as 5-6)**

**Objetivos:**
- ‚úÖ Implementar `useVideoCapture` hook
- ‚úÖ Implementar `AVSyncService`
- ‚úÖ Tests unitarios

**Tareas:**

```bash
[P3.1] Crear /src/hooks/useVideoCapture.ts
[P3.2] Implementar getUserMedia + MediaRecorder
[P3.3] Countdown autom√°tico (3-2-1)
[P3.4] Crear /src/services/avSyncService.ts
[P3.5] M√©todo scoreVideo() con FormData
[P3.6] Tests: captura ‚Üí env√≠o ‚Üí respuesta mock
```

**Criterios de Aceptaci√≥n:**
- Hook captura video+audio (640x480, 30fps, 4s)
- Retorna Blob v√°lido (WebM/VP9/Opus)
- Service env√≠a a API y parsea respuesta

---

### **Fase 4: Frontend - UI Components (D√≠as 7-9)**

**Objetivos:**
- ‚úÖ Crear modal `AVSyncChallengeModal`
- ‚úÖ Preview de c√°mara + countdown
- ‚úÖ Display de resultados

**Tareas:**

```bash
[P4.1] Crear /src/components/AVSyncChallengeModal.tsx
[P4.2] Estados: instructions ‚Üí countdown ‚Üí recording ‚Üí processing ‚Üí result
[P4.3] Crear VideoPreview.tsx (video element + overlay)
[P4.4] Crear ResultFeedback.tsx (‚úÖ‚ö†Ô∏è‚ùå + m√©tricas)
[P4.5] Animaciones con Framer Motion
[P4.6] Estilos con Tailwind (responsive)
[P4.7] Test en navegador (permisos, captura, UI)
```

**Criterios de Aceptaci√≥n:**
- Modal se abre/cierra correctamente
- Video preview funciona en Chrome/Firefox/Safari
- Countdown animado (3-2-1)
- Resultado muestra score + decisi√≥n + m√©tricas

---

### **Fase 5: Integraci√≥n con useLirvana (D√≠as 10-11)**

**Objetivos:**
- ‚úÖ Registrar tool `av_sync_challenge`
- ‚úÖ Conectar modal con orchestrator
- ‚úÖ Flujo completo E2E

**Tareas:**

```bash
[P5.1] Extender useLirvana: agregar estado isChallengeActive
[P5.2] Registrar funci√≥n av_sync_challenge en WebRTC
[P5.3] Handler handleChallengeComplete
[P5.4] Agregar tool definition en lirvanaTools.ts
[P5.5] Integrar modal en ChatBox.tsx
[P5.6] Test E2E: Agente ‚Üí Tool ‚Üí Modal ‚Üí Captura ‚Üí API ‚Üí Resultado ‚Üí Continuar
```

**Criterios de Aceptaci√≥n:**
- Agente puede ejecutar `av_sync_challenge` tool
- Modal se abre autom√°ticamente
- Early-exit funciona (ALLOW ‚Üí continuar conversaci√≥n)
- NEXT/BLOCK manejan flujos alternativos

---

### **Fase 6: Testing y Optimizaci√≥n (D√≠as 12-14)**

**Objetivos:**
- ‚úÖ Tests unitarios
- ‚úÖ Tests E2E
- ‚úÖ Calibraci√≥n de thresholds
- ‚úÖ Optimizaci√≥n de latencia

**Tareas:**

```bash
[P6.1] Dataset de prueba: 30 videos leg√≠timos + 20 deepfakes simulados
[P6.2] Benchmark: medir latencias (captura, API, Python)
[P6.3] Calibrar pesos de normalizaci√≥n (a, b, c)
[P6.4] Ajustar thresholds (0.90/0.75)
[P6.5] Tests de edge cases (sin cara, m√∫ltiples caras, ruido)
[P6.6] Tests de permisos denegados
[P6.7] Tests de error handling
[P6.8] Documentaci√≥n de m√©tricas
```

**Criterios de Aceptaci√≥n:**
- Rate de early-exit ‚â• 60% en leg√≠timos
- False positive rate < 5%
- p95 latencia < 10s
- Tests cubren 90% de casos

---

## 8. Testing y Validaci√≥n

### 8.1 Tests Unitarios

**Frontend:**

```typescript
// useVideoCapture.test.ts
describe('useVideoCapture', () => {
  it('should request permissions', async () => {
    const { result } = renderHook(() => useVideoCapture());
    await act(async () => {
      await result.current.requestPermissions();
    });
    expect(result.current.stream).not.toBeNull();
  });

  it('should capture 4s video', async () => {
    const { result } = renderHook(() => useVideoCapture({ duration: 4000 }));
    await act(async () => {
      await result.current.startCapture();
    });
    // Wait for completion
    await waitFor(() => expect(result.current.recordedBlob).not.toBeNull(), { timeout: 5000 });
    expect(result.current.recordedBlob?.type).toContain('video');
  });
});
```

**Backend:**

```javascript
// avsync.test.js
describe('POST /api/avsync/score', () => {
  it('should return score and decision', async () => {
    const formData = new FormData();
    formData.append('video', videoBlob, 'test.webm');
    formData.append('session_id', 'test123');
    formData.append('user_consent', 'true');

    const response = await fetch('http://localhost:3001/api/avsync/score', {
      method: 'POST',
      body: formData,
    });

    expect(response.status).toBe(200);
    const data = await response.json();
    expect(data).toHaveProperty('score');
    expect(data).toHaveProperty('decision');
    expect(data.decision).toMatch(/ALLOW|NEXT|BLOCK/);
  });
});
```

**Python:**

```python
# test_syncnet_wrapper.py
import pytest
from syncnet_wrapper import SyncNetWrapper

def test_process_video():
    wrapper = SyncNetWrapper(
        model_path='./models/syncnet_v2.model',
        detector_path='./models/sfd_face.pth'
    )

    result = wrapper.process_video('./test_data/sample.mp4', 'test_ref')

    assert 'offset_frames' in result
    assert 'confidence' in result
    assert 'score' in result
    assert 0 <= result['score'] <= 1
    assert result['processing_time_ms'] < 15000  # < 15s
```

### 8.2 Tests E2E

**Scenario 1: Happy Path (ALLOW)**

```gherkin
Feature: AV-Sync Challenge - Happy Path
  Scenario: Usuario leg√≠timo completa reto exitosamente
    Given el agente de voz est√° activo
    When el agente ejecuta tool "av_sync_challenge"
    Then el modal de captura se abre

    When el usuario acepta permisos de c√°mara
    Then el preview de video se muestra

    When el countdown llega a 0
    Then la grabaci√≥n inicia autom√°ticamente

    When el usuario habla la frase durante 4 segundos
    Then el video se procesa y env√≠a a API

    When la API responde con score=0.92 y decision="ALLOW"
    Then el modal muestra "‚úÖ Verificaci√≥n exitosa"
    And el modal se cierra despu√©s de 2 segundos
    And la conversaci√≥n contin√∫a normalmente
```

**Scenario 2: Edge Case (BLOCK)**

```gherkin
Feature: AV-Sync Challenge - Deepfake Detection
  Scenario: Detectar video con mala sincron√≠a (posible deepfake)
    Given el agente ejecuta tool "av_sync_challenge"
    When el usuario env√≠a un video con offset > 5 frames
    Then la API responde con score=0.65 y decision="BLOCK"
    And el modal muestra "‚ùå Alto riesgo detectado"
    And se ofrece opciones: [Reintentar] [Usar WebAuthn]
```

### 8.3 M√©tricas de Performance

**Latencias objetivo:**

| Etapa | p50 | p95 | p99 |
|-------|-----|-----|-----|
| **getUserMedia** | 500ms | 1.5s | 3s |
| **Captura (4s)** | 4s | 4.2s | 4.5s |
| **Upload ‚Üí Express** | 200ms | 800ms | 2s |
| **Python SyncNet** | 5s | 8s | 12s |
| **Respuesta ‚Üí UI** | 100ms | 300ms | 500ms |
| **TOTAL E2E** | 10s | 15s | 20s |

**Throughput:**
- M√°x. 5 requests concurrentes (limitado por Python service single-threaded)
- Con gunicorn (4 workers): hasta 20 req/s

---

## 9. Consideraciones T√©cnicas

### 9.1 Privacidad y Seguridad

**Datos sensibles:**
- ‚úÖ Video capturado NO se persiste por defecto (solo 24h opt-in con consentimiento)
- ‚úÖ Solo se almacenan m√©tricas: `{session_id, score, offset, timestamp}`
- ‚úÖ Archivos temporales en `/tmp` se limpian cada 24h (cron job)

**Consentimiento:**
```typescript
// Antes de capturar, mostrar banner:
"Esta verificaci√≥n captura un video corto (4s) de tu rostro y voz.
Los datos se procesan localmente y se eliminan inmediatamente despu√©s.
¬øAceptas continuar?"

[Aceptar] [Rechazar] [M√°s informaci√≥n]
```

**GDPR/CCPA Compliance:**
- Permitir opt-out del reto (usar m√©todo alternativo: WebAuthn)
- Derecho de borrado: endpoint `DELETE /api/avsync/data/:session_id`
- Logs anonimizados (hash de session_id)

### 9.2 Compatibilidad de Navegadores

| Navegador | getUserMedia | MediaRecorder | WebRTC | Soporte |
|-----------|--------------|---------------|--------|---------|
| **Chrome 90+** | ‚úÖ | ‚úÖ (VP9/Opus) | ‚úÖ | **Full** |
| **Firefox 88+** | ‚úÖ | ‚úÖ (VP8/Opus) | ‚úÖ | **Full** |
| **Safari 14+** | ‚úÖ | ‚ö†Ô∏è (MP4/H264) | ‚úÖ | **Partial** (requiere polyfill) |
| **Edge 90+** | ‚úÖ | ‚úÖ (VP9/Opus) | ‚úÖ | **Full** |
| **Mobile Chrome** | ‚úÖ | ‚úÖ | ‚úÖ | **Full** |
| **Mobile Safari** | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | **Limited** |

**Fallback para Safari:**
```typescript
const mimeType = MediaRecorder.isTypeSupported('video/webm;codecs=vp9,opus')
  ? 'video/webm;codecs=vp9,opus'
  : 'video/mp4'; // Safari fallback
```

### 9.3 Optimizaciones de Latencia

**1. Pre-warm del modelo SyncNet**

```python
# app.py
# Pre-cargar modelo al iniciar servidor (no lazy-load)
syncnet = SyncNetWrapper(...)  # Global, fuera de @app.route
```

**2. Compresi√≥n de video en navegador**

```typescript
const recorder = new MediaRecorder(stream, {
  mimeType: 'video/webm;codecs=vp9,opus',
  videoBitsPerSecond: 1500000, // 1.5 Mbps (reducir de 2.5 Mbps)
});
```

**3. Procesamiento paralelo en Python**

```python
# Si hay m√∫ltiples requests, usar ProcessPoolExecutor
from concurrent.futures import ProcessPoolExecutor

executor = ProcessPoolExecutor(max_workers=2)
future = executor.submit(syncnet.process_video, video_path, ref)
result = future.result(timeout=15)
```

**4. Cache de resultados (opcional)**

```python
# Redis cache para evitar re-procesar mismo video
# Key: sha256(video_blob)
# Value: {score, offset, confidence}
# TTL: 5 minutos
```

### 9.4 Escalabilidad (Futuro)

**Para producci√≥n con alta carga:**

1. **Containerizar Python service:**
   ```dockerfile
   # Dockerfile
   FROM python:3.10-slim
   RUN apt-get update && apt-get install -y ffmpeg
   COPY . /app
   WORKDIR /app
   RUN pip install -r requirements.txt
   CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
   ```

2. **Deploy con Docker Compose:**
   ```yaml
   # docker-compose.yml
   version: '3.8'
   services:
     express:
       build: ./server
       ports:
         - "3001:3001"
       depends_on:
         - python-syncnet

     python-syncnet:
       build: ./syncnet-service
       ports:
         - "5000:5000"
       volumes:
         - ./tmp:/app/tmp
       deploy:
         replicas: 2  # M√∫ltiples instancias
   ```

3. **Load balancer para Python:**
   - Nginx como reverse proxy
   - Round-robin entre r√©plicas

---

## 10. Roadmap y Timeline

### Sprint 1 (Semanas 1-2): Infraestructura + Backend

| Semana | D√≠as | Tareas | Entregable |
|--------|------|--------|-----------|
| **1** | 1-2 | Fase 1: Setup SyncNet + Python service | Python service running |
| **1** | 3-4 | Fase 2: Express API `/api/avsync/score` | API endpoint funcionando |
| **1** | 5 | Tests backend E2E | Postman collection |
| **2** | 6-7 | Fase 3: Frontend hook + service | `useVideoCapture` + `AVSyncService` |

**Checkpoint 1 (D√≠a 7):**
- ‚úÖ Python service procesa videos
- ‚úÖ Express API integrado
- ‚úÖ Hook captura video en navegador
- **Demo:** Captura manual ‚Üí API ‚Üí Resultado en consola

---

### Sprint 2 (Semanas 3-4): Frontend UI + Integraci√≥n

| Semana | D√≠as | Tareas | Entregable |
|--------|------|--------|-----------|
| **3** | 8-10 | Fase 4: Componentes UI (modal + preview + resultado) | UI completa |
| **3** | 11-12 | Fase 5: Integraci√≥n con `useLirvana` + tools | Flujo E2E funcionando |
| **4** | 13-14 | Fase 6: Testing + calibraci√≥n + optimizaci√≥n | MVP listo para demo |

**Checkpoint 2 (D√≠a 14):**
- ‚úÖ Modal funcional con countdown + captura + resultado
- ‚úÖ Agente puede ejecutar tool `av_sync_challenge`
- ‚úÖ Early-exit funciona (ALLOW ‚Üí continuar)
- ‚úÖ M√©tricas: p95 < 15s, early-exit rate > 60%
- **Demo completo:** Agente ‚Üí Reto ‚Üí An√°lisis ‚Üí Decisi√≥n ‚Üí Continuar

---

### Post-MVP (Semanas 5-6): Mejoras

| Feature | Prioridad | Esfuerzo |
|---------|-----------|----------|
| **Multiface detection** (quien habla) | Media | 5 d√≠as |
| **Ensemble con LipForensics** | Media | 7 d√≠as |
| **Calibraci√≥n con dataset real** | Alta | 3 d√≠as |
| **Dashboard de m√©tricas** (logs) | Baja | 3 d√≠as |
| **WebAuthn como fallback** | Alta | 5 d√≠as |
| **Modo offline (WASM)** | Baja | 14 d√≠as |

---

## 11. Riesgos y Mitigaciones

### Riesgo 1: Latencia de SyncNet > 10s

**Probabilidad:** Media
**Impacto:** Alto (UX degradada)

**Mitigaci√≥n:**
- Usar videos cortos (3-4s, no 6s)
- Pre-warm del modelo al iniciar
- GPU opcional (CUDA) para acelerar
- L√≠mite de timeout 15s con fallback a NEXT challenge

### Riesgo 2: False Positives (usuarios leg√≠timos bloqueados)

**Probabilidad:** Media
**Impacto:** Cr√≠tico (p√©rdida de usuarios)

**Mitigaci√≥n:**
- Calibrar thresholds con dataset real (30+ usuarios)
- Sistema de 3 niveles (ALLOW/NEXT/BLOCK) en vez de binario
- Permitir reintento (m√°x. 3 intentos)
- Fallback a WebAuthn si falla 3 veces

### Riesgo 3: Permisos de c√°mara denegados

**Probabilidad:** Media
**Impacto:** Medio (bloqueante)

**Mitigaci√≥n:**
- Mensaje claro de por qu√© se necesita c√°mara
- Opci√≥n de "Solo chat" (skip challenge)
- Fallback a m√©todo alternativo (email verification, SMS)

### Riesgo 4: Incompatibilidad Safari iOS

**Probabilidad:** Alta
**Impacto:** Medio (30% usuarios m√≥viles)

**Mitigaci√≥n:**
- Detecci√≥n de navegador + mensaje de compatibilidad
- Fallback a MP4/H264 para Safari
- Modo alternativo: captura de foto + audio separado

### Riesgo 5: Dependencia de ffmpeg local

**Probabilidad:** Baja
**Impacto:** Cr√≠tico (bloqueante)

**Mitigaci√≥n:**
- Documentar instalaci√≥n de ffmpeg en README
- Script de verificaci√≥n: `check-dependencies.sh`
- Dockerfile con ffmpeg pre-instalado para producci√≥n

### Riesgo 6: Dataset insuficiente para calibraci√≥n

**Probabilidad:** Alta
**Impacto:** Alto (precisi√≥n baja)

**Mitigaci√≥n:**
- Usar umbrales conservadores inicialmente (favor a ALLOW)
- Recopilar m√©tricas en producci√≥n (opt-in)
- Ajustar thresholds iterativamente cada 2 semanas

---

## 12. M√©tricas de Monitoreo

### KPIs de Producto

```yaml
Tasa de early-exit:
  - Objetivo: ‚â• 60%
  - F√≥rmula: (decisiones ALLOW) / (total retos)

False Positive Rate:
  - Objetivo: < 5%
  - F√≥rmula: (usuarios leg√≠timos bloqueados) / (total leg√≠timos)

Time to Complete:
  - Objetivo: p95 < 25s
  - Medir: desde tool execution hasta close modal

User Abandonment:
  - Objetivo: < 10%
  - F√≥rmula: (usuarios que cierran modal) / (total inicios)
```

### KPIs T√©cnicos

```yaml
API Latency:
  - p50: < 7s
  - p95: < 10s
  - p99: < 15s

Python Service Health:
  - Uptime: > 99.5%
  - Error rate: < 1%

Resource Usage:
  - CPU: < 70% promedio
  - RAM: < 4 GB promedio
  - Disk: /tmp < 5 GB
```

---

## 13. Documentaci√≥n Adicional

### README_SYNCNET.md

Crear gu√≠a de instalaci√≥n y uso del servicio Python:

```markdown
# SyncNet Service - Setup Guide

## Prerequisites
- Python 3.10+
- ffmpeg 5.x
- 4 GB RAM

## Installation
1. Clone SyncNet repo
2. Download models
3. Install dependencies
4. Run server

## API Reference
### POST /score
...

## Troubleshooting
...
```

### API_REFERENCE.md

Documentar contratos de API:

```markdown
# AVSync API Reference

## POST /api/avsync/score

### Request
- Content-Type: multipart/form-data
- Fields:
  - video: File (WebM/MP4, 2-6s)
  - session_id: string
  - user_consent: boolean

### Response
{
  "score": 0.91,
  "offset_frames": 3,
  "confidence": 10.02,
  "min_dist": 5.35,
  "lag_ms": 100,
  "decision": "ALLOW",
  "ttl_ms": 1698765432000
}
```

---

## 14. Ap√©ndice

### A. Referencias T√©cnicas

**SyncNet:**
- Repo oficial: https://github.com/joonson/syncnet_python
- Paper: "Out of time: automated lip sync in the wild" (Chung et al., 2016)
- VGG Models: https://www.robots.ox.ac.uk/~vgg/software/lipsync/

**MediaRecorder API:**
- MDN: https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder
- Browser support: https://caniuse.com/mediarecorder

**WebRTC:**
- getUserMedia: https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia

### B. Glosario

| T√©rmino | Definici√≥n |
|---------|-----------|
| **AV-Sync** | Audio-Visual Synchronization - sincron√≠a entre audio y movimiento labial |
| **Offset** | Desplazamiento temporal (en frames) entre audio y video |
| **Confidence** | M√©trica de confianza de SyncNet (mayor = mejor sincron√≠a) |
| **Min Dist** | Distancia m√≠nima en embedding space (menor = mejor match) |
| **Early-exit** | Terminar proceso anticipadamente cuando resultado es claro |
| **S3FD** | Single Shot Scale-invariant Face Detector (detector de rostros) |

### C. Comandos √ötiles

```bash
# Verificar instalaci√≥n ffmpeg
ffmpeg -version | head -n 1

# Test manual Python service
curl -X POST http://localhost:5000/score \
  -H "Content-Type: application/json" \
  -d '{"video_path": "/tmp/test.mp4", "session_id": "test"}'

# Monitorear logs Python
tail -f syncnet-service/app.log

# Limpiar archivos temporales
rm -rf /tmp/uploads/* /tmp/pywork/* /tmp/pycrop/*

# Benchmark latencia
time curl -X POST http://localhost:3001/api/avsync/score \
  -F "video=@test.webm" \
  -F "session_id=bench" \
  -F "user_consent=true"
```

---

## ‚úÖ Checklist de Aceptaci√≥n Final

Antes de considerar el MVP completo, verificar:

- [ ] Python service responde en `/health` y `/score`
- [ ] Express API `/api/avsync/score` funciona E2E
- [ ] Hook `useVideoCapture` captura video correctamente
- [ ] Modal `AVSyncChallengeModal` muestra todos los estados
- [ ] Tool `av_sync_challenge` registrado en `useLirvana`
- [ ] Flujo completo: Agente ‚Üí Tool ‚Üí Modal ‚Üí Captura ‚Üí API ‚Üí Resultado
- [ ] Early-exit (ALLOW) contin√∫a conversaci√≥n sin interrupciones
- [ ] Decisiones NEXT/BLOCK manejan flujos alternativos
- [ ] p95 latencia < 15s
- [ ] Early-exit rate > 60% en tests internos
- [ ] False positive rate < 5%
- [ ] Compatible con Chrome/Firefox/Edge
- [ ] Permisos de c√°mara manejados correctamente
- [ ] Error handling robusto (sin crashes)
- [ ] Logging detallado en todas las capas
- [ ] Documentaci√≥n actualizada (README, API_REFERENCE)
- [ ] Tests unitarios pasan (frontend + backend + python)
- [ ] Demo grabado para stakeholders

---

## üéØ Pr√≥ximos Pasos

1. **Revisi√≥n del Plan** - Validar con equipo y stakeholders
2. **Setup de Ambiente** - Instalar dependencias y verificar compatibilidad
3. **Kickoff Sprint 1** - Comenzar con Fase 1 (Setup SyncNet)
4. **Checkpoint Semanal** - Review de progreso y ajustes

---

**Elaborado por:** Claude Code
**Fecha:** 2025-11-01
**Versi√≥n:** 1.0
**Estado:** ‚úÖ Listo para Revisi√≥n

---

**Notas finales:**

Este plan est√° dise√±ado para ejecutarse completamente en modo **local** sin dependencias cloud, como se solicit√≥. Todas las piezas (Express, Python, SyncNet) corren en `localhost` y pueden desplegarse en un solo servidor o m√°quina de desarrollo.

Para producci√≥n, se recomienda containerizar con Docker y usar un load balancer, pero el MVP funciona perfectamente en modo local para demos y pruebas.

¬øListo para implementar? üöÄ
